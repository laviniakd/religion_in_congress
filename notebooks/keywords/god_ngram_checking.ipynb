{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "NGRAM_GOD_PATH = \"/home/laviniad/projects/religion_in_congress/notebooks/god_n_grams.pkl\"\n",
    "\n",
    "with open(NGRAM_GOD_PATH, \"rb\") as f:\n",
    "    sorted_god_n_grams = pkl.load(f)\n",
    "\n",
    "only_god_in_central = [(k, v) for k, v in sorted_god_n_grams if k.split(\" \")[2] == \"God\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f6d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ngram_insts = []\n",
    "for n_gram, count in only_god_in_central:\n",
    "    for i in range(count):\n",
    "        ngram_insts.append(n_gram)\n",
    "\n",
    "ngram_insts = pd.Series(ngram_insts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f592c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of god uses are in top 10, 25, 50?\n",
    "top_10 = sorted_god_n_grams[:10]\n",
    "top_25 = sorted_god_n_grams[:25]\n",
    "top_50 = sorted_god_n_grams[:50]\n",
    "top_100 = sorted_god_n_grams[:100]\n",
    "top_250 = sorted_god_n_grams[:250]\n",
    "\n",
    "print(\"Top 10:\")\n",
    "perc_in_top_10 = round(sum([count for _, count in top_10]) / sum([count for _, count in sorted_god_n_grams]) * 100, 3)\n",
    "print(f\"{perc_in_top_10}% of all uses are in top 10\")\n",
    "\n",
    "print(\"Top 25:\")\n",
    "perc_in_top_25 = round(sum([count for _, count in top_25]) / sum([count for _, count in sorted_god_n_grams]) * 100, 3)\n",
    "print(f\"{perc_in_top_25}% of all uses are in top 25\")\n",
    "\n",
    "print(\"Top 50:\")\n",
    "perc_in_top_50 = round(sum([count for _, count in top_50]) / sum([count for _, count in sorted_god_n_grams]) * 100, 3)\n",
    "print(f\"{perc_in_top_50}% of all uses are in top 50\")\n",
    "\n",
    "print(\"Top 100:\")\n",
    "perc_in_top_100 = round(sum([count for _, count in top_100]) / sum([count for _, count in sorted_god_n_grams]) * 100, 3)\n",
    "print(f\"{perc_in_top_100}% of all uses are in top 100\")\n",
    "\n",
    "print(\"Top 250:\")\n",
    "perc_in_top_250 = round(sum([count for _, count in top_250]) / sum([count for _, count in sorted_god_n_grams]) * 100, 3)\n",
    "print(f\"{perc_in_top_250}% of all uses are in top 250\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8163a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def entropy(counter):\n",
    "    total = sum(counter.values())\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    return -sum((c/total) * math.log2(c/total) for c in counter.values())\n",
    "\n",
    "\n",
    "def get_entropy_of_word(texts, target_word):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the words surrounding a target word in a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of strings (texts).\n",
    "        target_word (str): The target word to analyze.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Entropy of left and right contexts.\n",
    "    \"\"\"\n",
    "    lefts = Counter()\n",
    "    rights = Counter()\n",
    "        \n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == target_word:\n",
    "                if i > 0:\n",
    "                    lefts[tokens[i-1]] += 1\n",
    "                if i < len(tokens) - 1:\n",
    "                    rights[tokens[i+1]] += 1\n",
    "        \n",
    "    left_entropy = entropy(lefts)\n",
    "    right_entropy = entropy(rights)\n",
    "\n",
    "    return left_entropy, right_entropy, lefts, rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b85608",
   "metadata": {},
   "outputs": [],
   "source": [
    "lentropy, rentropy, lefts, rights = get_entropy_of_word([text for text, _ in only_god_in_central], \"God\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e601c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_entropy(entropy, num_contexts):\n",
    "    \"\"\"\n",
    "    Normalize the entropy value based on the number of contexts.\n",
    "    \n",
    "    Args:\n",
    "        entropy (float): The entropy value to normalize.\n",
    "        num_contexts (int): The number of contexts.\n",
    "        \n",
    "    Returns:\n",
    "        float: Normalized entropy value.\n",
    "    \"\"\"\n",
    "    return entropy / math.log2(num_contexts) if num_contexts > 0 else 0\n",
    "\n",
    "\n",
    "norm_lentropy = normalized_entropy(lentropy, len(lefts))\n",
    "norm_rentropy = normalized_entropy(rentropy, len(rights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "962683f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5856343953089923, 0.5623877854387346)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_lentropy, norm_rentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c80aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sermons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
