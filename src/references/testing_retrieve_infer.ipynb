{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428db3a0-4b7d-4461-b6d2-1709a342f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549ae9a3-10a1-49da-9cce-5ce7a3941c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading congressional data\n",
      "On file 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 106493/106493 [00:00<00:00, 170303.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 190933/190933 [00:01<00:00, 175686.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 94162/94162 [00:00<00:00, 206183.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 207487/207487 [00:00<00:00, 216271.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 183503/183503 [00:00<00:00, 215197.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 130265/130265 [00:00<00:00, 228412.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 197548/197548 [00:00<00:00, 215442.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 7\n",
      "On file 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 139281/139281 [00:00<00:00, 225796.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 127268/127268 [00:00<00:00, 214713.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 179989/179989 [00:00<00:00, 211750.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 271528/271528 [00:01<00:00, 225796.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 48007/48007 [00:00<00:00, 237146.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 210379/210379 [00:01<00:00, 207644.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 196111/196111 [00:00<00:00, 221269.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On file 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:00<00:00, 218453.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In debug mode; sampled 10000 documents from congressional record\n",
      "Loading keywords\n",
      "Synonym set: ['God', 'God Almighty', 'Supreme being', 'Lord', 'Divine', 'supreme being', 'Creator', 'god almighty', 'Almighty', 'godhead', 'Godhead', 'jehovah', 'Jehovah', 'almighty', 'god', 'Maker', 'divine']\n",
      "Filtering congressional data by keywords\n",
      "Number of religious speeches: 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [00:00, 2034.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of potentially religious sentences: 6297\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading verses...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rapidfuzz import fuzz, process\n",
    "from data.bible_utils import comp_bible_helper\n",
    "from data.congress_utils import induce_party_and_state\n",
    "from data.data_utils import load_god_synonyms, get_lexical_overlap\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from data import congress_utils\n",
    "from src.references.train_biencoder import BiModel\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--model_dir', default=\"/data/laviniad/sermons-ir/modeling/tuned_mpnet/model.zip\", type=str)\n",
    "#parser.add_argument('--input', default=\"/data/corpora/congressional-record/\", type=str)\n",
    "#parser.add_argument('--out_dir', default=\"/data/laviniad/sermons-ir/modeling/mpnet_results/\")\n",
    "#parser.add_argument('--congress_errata_path', default=\"/data/laviniad/congress_errata/\", type=str)\n",
    "#parser.add_argument('--device', default=\"2\", type=str)\n",
    "#parser.add_argument('--debug', action=\"store_true\")\n",
    "#args = parser.parse_args()\n",
    "args = {'model_dir': \"/data/laviniad/sermons-ir/modeling/tuned_mpnet/model.zip\",\n",
    "        'input': \"/data/corpora/congressional-record/\",\n",
    "        'out_dir': \"/data/laviniad/sermons-ir/modeling/mpnet_results/\",\n",
    "        'congress_errata_path': \"/data/laviniad/congress_errata/\",\n",
    "        'device': \"2\",\n",
    "        'debug': True\n",
    "       }\n",
    "\n",
    "DEVICE = 'cuda:' + args['device'] if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "KEYWORD_FILTER_THRESHOLD = 0.0005\n",
    "\n",
    "# load congressional data\n",
    "print(\"Loading congressional data\")\n",
    "congressional_df = congress_utils.load_full_df_from_raw(args['input'])\n",
    "congressional_df = induce_party_and_state(congressional_df)\n",
    "\n",
    "if args['debug']:\n",
    "    congressional_df = congressional_df.sample(10000)\n",
    "    print(\"In debug mode; sampled 10000 documents from congressional record\")\n",
    "\n",
    "# load keywords\n",
    "print(\"Loading keywords\")\n",
    "keywords_path = '/home/laviniad/projects/religion_in_congress/src/multi-feature-use/'\n",
    "keyword_strs = keywords_path + 'keywords_from_coca.txt', keywords_path + 'keywords_from_congress.txt'\n",
    "\n",
    "def get_keywords(keyword_path):\n",
    "    with open(keyword_path) as f:\n",
    "        keyword_set = [l.strip() for l in f.readlines()]\n",
    "    return keyword_set\n",
    "\n",
    "keywords_coca = get_keywords(keyword_strs[0])\n",
    "keywords_congress = get_keywords(keyword_strs[1])\n",
    "\n",
    "god_synonyms = load_god_synonyms()\n",
    "full_keywords = list(set(keywords_coca).intersection(set(keywords_congress)).union(set([t.capitalize() for t in god_synonyms])))\n",
    "full_keywords.remove('flesh')\n",
    "temp = []\n",
    "for i in full_keywords:\n",
    "    if not ('god' in i and i != 'god'):\n",
    "        temp.append(i) # avoid all lowercased god mentions\n",
    "\n",
    "full_keywords = temp\n",
    "words_of_concern = ['verse', 'verses', 'thou', 'Verse', 'Verses', 'Thou'] # not good signals\n",
    "for w in words_of_concern:\n",
    "    if w in full_keywords:\n",
    "        full_keywords.remove(w)\n",
    "\n",
    "# retrieve the speeches that contain these keywords and create new df with rows containing sentence + verse + sermon idx of sentence\n",
    "print(\"Filtering congressional data by keywords\")\n",
    "def filter(speech, threshold):\n",
    "    overlap = get_lexical_overlap(speech, full_keywords)\n",
    "    return overlap > threshold\n",
    "\n",
    "congressional_df['religious'] = congressional_df['text'].apply(lambda x: filter(x, KEYWORD_FILTER_THRESHOLD))\n",
    "filtered_df = congressional_df[congressional_df['religious']]\n",
    "print(f\"Number of religious speeches: {len(filtered_df.index)}\")\n",
    "\n",
    "infer_df = []\n",
    "for idx, row in tqdm(filtered_df.iterrows()):\n",
    "    sentence_list = nltk.sent_tokenize(row['text'])\n",
    "    for i,s in enumerate(sentence_list):\n",
    "        infer_df.append({\n",
    "            'sermon_idx': idx,\n",
    "            'index_in_sermon': i,\n",
    "            'text': s\n",
    "        })\n",
    "\n",
    "infer_df = pd.DataFrame(infer_df)\n",
    "print(f\"Number of potentially religious sentences: {len(infer_df.index)}\")\n",
    "\n",
    "# load model\n",
    "print(\"Loading model...\")\n",
    "full_model = BiModel(device=DEVICE).to(DEVICE)\n",
    "full_model.load_state_dict(torch.load(args['model_dir']))\n",
    "model = full_model.model\n",
    "model.eval()\n",
    "biTokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# load verses\n",
    "print(\"Loading verses...\")\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "bible_df = comp_bible_helper()\n",
    "pop_verses = pd.read_csv('/home/laviniad/projects/religion_in_congress/data/most_popular_verses.csv')\n",
    "n = 250 # VERY generous\n",
    "pop_citations = list(pop_verses['verse'].iloc[1:n+1])\n",
    "bible_df['King James Bible'] = bible_df['King James Bible'].apply(remove_tags) # KJV in this df has italics etc\n",
    "bible_df['Verse'] = bible_df['Verse'].apply(lambda x: x.lower())\n",
    "limited_bible_df = bible_df[bible_df['Verse'].apply(lambda x: x in pop_citations)]\n",
    "limited_verses = limited_bible_df['King James Bible']\n",
    "verse_df = [{'text': t['King James Bible'], 'citation': t['citation']} for idx,t in limited_bible_df.iterrows()]\n",
    "limited_verse_to_citation = dict(zip(limited_verses, limited_bible_df['Verse']))\n",
    "limited_citation_to_verse = {v.lower(): k for k,v in limited_verse_to_citation.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8592c42f-a0fc-49af-9cc8-d318a4219d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Congress data loader\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embed congress sentences\n",
    "print(\"Creating Congress data loader\")\n",
    "congressDataset = Dataset.from_pandas(infer_df)\n",
    "congressDataset = congressDataset.map(lambda x: biTokenizer(x[\"text\"], max_length=512, padding=\"max_length\", truncation=True))\n",
    "        \n",
    "for col in ['input_ids', 'attention_mask']:\n",
    "    congressDataset = congressDataset.rename_column(col, 'text'+'_'+col)\n",
    "            \n",
    "congressDataset.set_format(type='torch')\n",
    "congressLoader = torch.utils.data.DataLoader(congressDataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fddf6bd-714d-4920-ba41-98727b384412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Congress sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                    | 2/1575 [00:00<01:36, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([2133635, 2133635, 2133635, 2133635]), 'index_in_sermon': tensor([0, 1, 2, 3]), 'text': ['Mr. JONES of North Carolina.', 'Mr. Speaker, I thank the gentleman for yielding me the time, and I want to thank the chairman of this committee and the ranking member for working with me on H.R.', '1799, the Fallen Heroes Immigrant Spouse Fairness Act.', 'Mr. Speaker, this came to my attention when I attended the funeral of a Marine who was killed in Operation Iraqi Freedom.'], 'text_input_ids': tensor([[    0,  2724,  1016,  ...,     1,     1,     1],\n",
      "        [    0,  2724,  1016,  ...,     1,     1,     1],\n",
      "        [    0, 13843,  1014,  ...,     1,     1,     1],\n",
      "        [    0,  2724,  1016,  ...,     1,     1,     1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([2133635, 2133635, 2133635, 2133635]), 'index_in_sermon': tensor([4, 5, 6, 7]), 'text': ['His name was Michael Bitz.', 'Sergeant Bitz was married to a lady, Janina Bitz, who was from Australia, and at the time we were concerned with the fact that he had lost his life, that his wife might have to start the process again of becoming a naturalized citizen.', 'When I attended the funeral down at Camp Lejeune of Sergeant Bitz, I met Pat Millush, the military liaison to the Bureau of Citizenship and Immigration Service at Camp Lejeune.', 'Pat said to me the immigrant spouses of military personnel were treated unfairly under current immigration law.'], 'text_input_ids': tensor([[   0, 2014, 2175,  ...,    1,    1,    1],\n",
      "        [   0, 6726, 2982,  ...,    1,    1,    1],\n",
      "        [   0, 2047, 1049,  ...,    1,    1,    1],\n",
      "        [   0, 6990, 2060,  ...,    1,    1,    1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([2133635, 2133635, 2133635, 2133635]), 'index_in_sermon': tensor([ 8,  9, 10, 11]), 'text': ['By knowing that, Mr. Speaker, I decided that I would put this legislation in that would allow the spouse of a member of the military who had lost their life, whether it be in war or by accident or in training, that if they had not reached that 2-year period of time, that they would still be able to continue the naturalization process without being penalized.', 'I am delighted and want to thank again the chairman of the committee and the ranking member for not only working with me on this issue but other Members who have been named today, because the men and women who serve this great Nation and their families need to be honored; and I think this bill itself is a way to honor those who have given their lives for this great Nation.', 'Basically what 1799 did, which has been included in this bill, allows the immigrant spouse of military personnel who die as a result of a service-connected injury or disease to continue the immigration process regardless of the number of years of the marriage.', 'Mr. Speaker, I have outside of my office, 422 Cannon, a photograph of everyone who has died in the war for freedom in Iraq, and I am pleased and honored that this committee would accept the language in 1799 and encompass it in this naturalization bill to honor our men and women in uniform.'], 'text_input_ids': tensor([[    0,  2015,  4213,  ...,     1,     1,     1],\n",
      "        [    0,  1049,  2576,  ...,     1,     1,     1],\n",
      "        [    0, 10472,  2058,  ...,     1,     1,     1],\n",
      "        [    0,  2724,  1016,  ...,     1,     1,     1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([2133635, 2133635, 1336926, 1336926]), 'index_in_sermon': tensor([12, 13,  0,  1]), 'text': ['Mr. Speaker, again, I will close by saying I ask God to please bless our men and women in uniform.', 'I ask God to please bless the families of the loved ones fighting for freedom; and again, I thank the leadership, the Republican leadership and the Democratic leadership, for this honor that they have given to Michael Bitz who gave his life for America.', 'The SPEAKER pro tempore.', 'Will the gentleman from South Carolina (Mr. Wilson) come forward and lead the House in the Pledge of Allegiance.'], 'text_input_ids': tensor([[   0, 2724, 1016,  ...,    1,    1,    1],\n",
      "        [   0, 1049, 3202,  ...,    1,    1,    1],\n",
      "        [   0, 2000, 5886,  ...,    1,    1,    1],\n",
      "        [   0, 2101, 2000,  ...,    1,    1,    1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Embeddings: torch.Size([4, 512, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                                    | 8/1575 [00:00<01:16, 20.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([1336926,   51815,   51815,   51815]), 'index_in_sermon': tensor([2, 0, 1, 2]), 'text': ['Mr. WILSON of South Carolina led the Pledge of Allegiance as follows: I pledge allegiance to the Flag of the United States of America, and to the Republic for which it stands, one nation under God, indivisible, with liberty and justice for all.', 'Mr. JOHNSON of Louisiana.', \"Mr. Chairman, I appreciate my colleague's presentation here.\", 'It is clear and concise, and he raises important points.'], 'text_input_ids': tensor([[   0, 2724, 1016,  ...,    1,    1,    1],\n",
      "        [   0, 2724, 1016,  ...,    1,    1,    1],\n",
      "        [   0, 2724, 1016,  ...,    1,    1,    1],\n",
      "        [   0, 2013, 2007,  ...,    1,    1,    1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([51815, 51815, 51815, 51815]), 'index_in_sermon': tensor([3, 4, 5, 6]), 'text': ['Mr. Chairman, I rise in opposition to H.R.', '2181, the Chaco Cultural Heritage Area Protection Act, as well.', 'Mr. Chairman, this is a flawed bill.', 'It is simply another attempt by our colleagues on the other side of the aisle to prevent our country from taking the next steps in this era of American energy dominance.'], 'text_input_ids': tensor([[    0,  2724,  1016,  ...,     1,     1,     1],\n",
      "        [    0, 20745,  2491,  ...,     1,     1,     1],\n",
      "        [    0,  2724,  1016,  ...,     1,     1,     1],\n",
      "        [    0,  2013,  2007,  ...,     1,     1,     1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([51815, 51815, 51815, 51815]), 'index_in_sermon': tensor([ 7,  8,  9, 10]), 'text': ['What is important here is that American energy dominance is a great strategy.', 'It is a strategy that helps all Americans, those in this immediate area and around the country.', 'The legislation before us will, of course, permanently restrict oil and gas development in the area immediately surrounding the Chaco Culture National Historical Park.', 'Now, bear in mind, of course, as has been pointed out here, exploration is already restricted within the park; and, of course, that is rightfully so.'], 'text_input_ids': tensor([[   0, 2058, 2007,  ...,    1,    1,    1],\n",
      "        [   0, 2013, 2007,  ...,    1,    1,    1],\n",
      "        [   0, 2000, 6098,  ...,    1,    1,    1],\n",
      "        [   0, 2089, 1014,  ...,    1,    1,    1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([51815, 51815, 51815, 51815]), 'index_in_sermon': tensor([11, 12, 13, 14]), 'text': ['But it is bad policy to create an arbitrary buffer zone for a prohibition on development in the area around the park.', \"In this Congress, our friends on the other side of the aisle have made their priorities crystal clear regarding the management of our country's resources.\", 'So far, they have placed moratoriums on oil and gas production in the eastern Gulf of Mexico, in the Pacific and Atlantic planning areas, and in ANWR.', 'Apparently, that is not enough.'], 'text_input_ids': tensor([[   0, 2025, 2013,  ...,    1,    1,    1],\n",
      "        [   0, 2003, 2027,  ...,    1,    1,    1],\n",
      "        [   0, 2065, 2525,  ...,    1,    1,    1],\n",
      "        [   0, 4597, 1014,  ...,    1,    1,    1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([51815, 51815, 51815, 51815]), 'index_in_sermon': tensor([15, 16, 17, 18]), 'text': ['What we are hearing today is that now we need to ban production in the New Mexico areas, as well.', 'Mr. Chairman, at what point do we say enough is enough?', 'The evidence shows, time and again, that placing restrictions on energy development only increases prices for American consumers.', 'And make no mistake, these increases have the largest impact on our most vulnerable communities.'], 'text_input_ids': tensor([[   0, 2058, 2061,  ...,    1,    1,    1],\n",
      "        [   0, 2724, 1016,  ...,    1,    1,    1],\n",
      "        [   0, 2000, 3354,  ...,    1,    1,    1],\n",
      "        [   0, 2002, 2195,  ...,    1,    1,    1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                                   | 10/1575 [00:00<01:23, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: torch.Size([4, 512, 768])\n",
      "Shape of mean pooled: torch.Size([4, 768])\n",
      "Batch: {'sermon_idx': tensor([51815, 51815, 51815, 51815]), 'index_in_sermon': tensor([19, 20, 21, 22]), 'text': ['I said this on the floor in September--many of us have--and I will say it again today, the United States is blessed because our land is filled with an abundance of natural resources.', 'My own congressional district back in Louisiana is home of one of the largest natural gas reserves in the country.', 'We believe, we insist that we have the means and the responsibility to use those God-given resources to create jobs, foster economic growth, and pave the way to an era of American energy dominance.', 'Oppressive policies like the ones before us today have been our own worst enemy.'], 'text_input_ids': tensor([[    0,  1049,  2060,  ...,     1,     1,     1],\n",
      "        [    0,  2030,  2223,  ...,     1,     1,     1],\n",
      "        [    0,  2061,  2907,  ...,     1,     1,     1],\n",
      "        [    0, 28562,  6047,  ...,     1,     1,     1]]), 'text_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 2; 23.69 GiB total capacity; 21.93 GiB already allocated; 3.69 MiB free; 22.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      5\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 6\u001b[0m embedding \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_masks)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m mean_pooled \u001b[38;5;241m=\u001b[39m full_model\u001b[38;5;241m.\u001b[39mmean_pooling(embedding, attention_masks)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:551\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    550\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds)\n\u001b[0;32m--> 551\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    552\u001b[0m     embedding_output,\n\u001b[1;32m    553\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    554\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    555\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    556\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    557\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    560\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:341\u001b[0m, in \u001b[0;36mMPNetEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    339\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 341\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    342\u001b[0m     hidden_states,\n\u001b[1;32m    343\u001b[0m     attention_mask,\n\u001b[1;32m    344\u001b[0m     head_mask[i],\n\u001b[1;32m    345\u001b[0m     position_bias,\n\u001b[1;32m    346\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:300\u001b[0m, in \u001b[0;36mMPNetLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    293\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    299\u001b[0m ):\n\u001b[0;32m--> 300\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    301\u001b[0m         hidden_states,\n\u001b[1;32m    302\u001b[0m         attention_mask,\n\u001b[1;32m    303\u001b[0m         head_mask,\n\u001b[1;32m    304\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m    305\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    306\u001b[0m     )\n\u001b[1;32m    307\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    308\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:241\u001b[0m, in \u001b[0;36mMPNetAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    234\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    240\u001b[0m ):\n\u001b[0;32m--> 241\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    242\u001b[0m         hidden_states,\n\u001b[1;32m    243\u001b[0m         attention_mask,\n\u001b[1;32m    244\u001b[0m         head_mask,\n\u001b[1;32m    245\u001b[0m         position_bias,\n\u001b[1;32m    246\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(self_outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m hidden_states)\n\u001b[1;32m    249\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:177\u001b[0m, in \u001b[0;36mMPNetSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(v)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    178\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Apply relative position embedding (precomputed in MPNetEncoder) if provided.\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 2; 23.69 GiB total capacity; 21.93 GiB already allocated; 3.69 MiB free; 22.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print(\"Embedding Congress sentences\")\n",
    "congress_result_tuples = []\n",
    "for batch in tqdm(congressLoader):\n",
    "    input_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "    attention_masks = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "    embedding = model(input_ids, attention_mask=attention_masks)[0]\n",
    "    print(f\"Embeddings: {embedding.size()}\")\n",
    "    mean_pooled = full_model.mean_pooling(embedding, attention_masks).to('cpu')\n",
    "    print(f\"Shape of mean pooled: {mean_pooled.size()}\")\n",
    "    print(f\"Batch: {batch}\")\n",
    "    for b in range(0, 4):\n",
    "        print(b)\n",
    "        congress_result_tuples.append((batch['text'][b], batch['sermon_idx'][b], mean_pooled[b]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab213b33-09c3-4fbf-aa52-dcb8db5adc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Congress sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                             | 0/1575 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      5\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 6\u001b[0m embedding \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_masks)[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m mean_pooled \u001b[38;5;241m=\u001b[39m BiModel\u001b[38;5;241m.\u001b[39mmean_pooling(embedding, attention_masks)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of embedding batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/transformers/utils/generic.py:335\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_dict[k]\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tuple()[k]\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# embed bible verses\n",
    "print(\"Creating Bible verse data loader\")\n",
    "verseDataset = Dataset.from_pandas(verse_df)\n",
    "verseDataset = verseDataset.map(lambda x: biTokenizer(x[\"text\"], max_length=512, padding=\"max_length\", truncation=True))\n",
    "        \n",
    "for col in ['input_ids', 'attention_mask']:\n",
    "    verseDataset = verseDataset.rename_column(col, 'text'+'_'+col)\n",
    "            \n",
    "verseDataset.set_format(type='torch')\n",
    "verseLoader = torch.utils.data.DataLoader(verseDataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Embedding Bible verses\")\n",
    "verse_result_tuples = []\n",
    "for batch in tqdm(verseLoader):\n",
    "    input_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "    attention_masks = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "    embedding = model(input_ids, attention_mask=attention_masks)[:,0]\n",
    "    mean_pooled = BiModel.mean_pooling(embedding, attention_masks).to('cpu')\n",
    "    print(f\"Shape of embedding batch: {embedding.size()}\")\n",
    "    for b in range(0, len(batch)):\n",
    "        verse_result_tuples.append((batch[b]['text'], batch[b]['citation'], embedding[b]))\n",
    "\n",
    "# create new df of references given the above pairs\n",
    "result_df = []\n",
    "print(\"Finding most similar Bible verses\")\n",
    "for congress_tuple in tqdm(congress_result_tuples):\n",
    "    embedding = congress_tuple[2]\n",
    "    \n",
    "    similarities = [1 - cosine(embedding, verse[2]) for verse in verse_result_tuples]\n",
    "    max_similarity_index = np.argmax(similarities)\n",
    "    cosine_sim = similarities[max_similarity_index]\n",
    "    verse_tuple = verse_result_tuples[max_similarity_index]\n",
    "    result_df.append({\n",
    "        'sermon_idx': congress_tuple[1], # comes from congress_utils.load_full_df_from_raw(args.input) indices\n",
    "        'text': congress_tuple[0],\n",
    "        'most_similar_verse': verse_tuple[1],\n",
    "        'cosine_similarity': cosine_sim,\n",
    "        'verse_citation': verse_tuple[1],\n",
    "    })\n",
    "\n",
    "# dump\n",
    "if not args['debug']:\n",
    "    result_df.to_csv(args['out_dir'] + 'results.csv')\n",
    "    print(f\"Dumped result_df to {args['out_dir'] + 'results.csv'}\")\n",
    "else:\n",
    "    print(\"In debug mode; did not dump results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f2e94-d28e-4a5b-80d8-b8af1f04ada3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sermons",
   "language": "python",
   "name": "sermons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
