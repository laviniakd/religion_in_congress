{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c2e5e5b-4b8f-48c7-9f9b-2f3d9de95394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data import presidential_utils\n",
    "from data.data_utils import load_spacy_sentencizer\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "from spacy.pipeline import Sentencizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "config = {\"punct_chars\": ['!', '.', '?', '...', ';', ':', '(', ')']}\n",
    "nlp.add_pipe(\"sentencizer\", config=config)\n",
    "from spacy.lang.en import stop_words\n",
    "stop_words = stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe47fb11-5c88-4916-9143-7bbb29e0719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████▊                                                     | 19/98 [00:05<00:21,  3.75it/s]\n"
     ]
    }
   ],
   "source": [
    "pres_path = \"/data/laviniad/presidential\"\n",
    "\n",
    "print(\"Now loading data...\")\n",
    "presidential_df = presidential_utils.load_full_df_from_raw(pres_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcd8bc21-2167-4b67-a061-829c9a7ce6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load models etc\n",
    "import json\n",
    "import pickle as pkl\n",
    "import pprint\n",
    "\n",
    "kc_keywords_path = '/home/laviniad/projects/religion_in_congress/src/multi-feature-use/kevincoekeywords/full.json'\n",
    "keyword_strs = 'keywords_from_coca.txt', 'keywords_from_congress.txt'\n",
    "\n",
    "def get_keywords(keyword_path):\n",
    "    with open(keyword_path) as f:\n",
    "        keyword_set = [l.strip() for l in f.readlines()]\n",
    "    return keyword_set\n",
    "\n",
    "keywords_coca = get_keywords(keyword_strs[0])\n",
    "keywords_congress = get_keywords(keyword_strs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1431bd5a-218f-470a-a08e-76261c4c4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(kc_keywords_path, 'r') as f:\n",
    "    kc_keywords = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5fe53a5-df3f-4b73-b072-8bec79b3d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecf6f2fe-7797-463e-a70d-271c9382db69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym set: ['Creator', 'Lord', 'almighty', 'Divine', 'god', 'divine', 'Jehovah', 'Supreme being', 'jehovah', 'supreme being', 'god almighty', 'godhead', 'Maker', 'God', 'God Almighty', 'Almighty', 'Godhead']\n"
     ]
    }
   ],
   "source": [
    "TOK = ['god']\n",
    "wnl = WordNetLemmatizer()\n",
    "syn = wn.synsets(wnl.lemmatize(TOK[0]), pos=wn.NOUN)\n",
    "actual = syn[0]\n",
    "hyponyms = actual.instance_hyponyms()[:-1] # last tends to be... sketch\n",
    "lemmas_of_hyponyms = [s.lemmas() for s in hyponyms]\n",
    "hyponyms = [str(s.name()) for instance in lemmas_of_hyponyms for s in instance]\n",
    "TOK += hyponyms\n",
    "TOK += [str(s.name()).capitalize() for s in actual.lemmas()]\n",
    "TOK = list(set([s.replace('_', ' ') for s in TOK if s != 'Allah']))\n",
    "temp = []\n",
    "for t in TOK:\n",
    "    if t != 'Creator' and t != 'Maker' and t!= 'Lord':\n",
    "        temp.append(t.lower())\n",
    "TOK += temp\n",
    "TOK = list(set(TOK))\n",
    "print(f\"Synonym set: {TOK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7baa612-bd7a-42ee-8ebc-e79f1a45cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MY_KEYWORDS = True\n",
    "\n",
    "if USE_MY_KEYWORDS:\n",
    "    full_keywords = list(set(keywords_coca).intersection(set(keywords_congress)).union(set([t.lower() for t in TOK])))\n",
    "    full_keywords.remove('god almighty')\n",
    "    full_keywords.remove('flesh')\n",
    "    full_keywords.remove('apostles')\n",
    "    temp = []\n",
    "    for i in full_keywords:\n",
    "        if not ('god' in i and i != 'god'):\n",
    "            temp.append(i)\n",
    "\n",
    "    full_keywords = temp\n",
    "else: \n",
    "    full_keywords = [e[1] for e in kc_keywords.items()]\n",
    "    full_keywords = [' ' + e + ' ' for s in full_keywords for e in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "314f5860-f44c-4f19-b71b-dd2821a12be9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m keyword_temp \u001b[38;5;241m=\u001b[39m full_keywords\n\u001b[1;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec-google-news-300\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m gensim\u001b[38;5;241m.\u001b[39mdownloader\u001b[38;5;241m.\u001b[39mload(model_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/gensim/downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, BASE_DIR)\n\u001b[1;32m    502\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(name)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mload_data()\n",
      "File \u001b[0;32m~/gensim-data/word2vec-google-news-300/__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[1;32m      7\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec-google-news-300\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec-google-news-300.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload_word2vec_format(path, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39mfvocab, binary\u001b[38;5;241m=\u001b[39mbinary, encoding\u001b[38;5;241m=\u001b[39mencoding, unicode_errors\u001b[38;5;241m=\u001b[39municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[38;5;241m=\u001b[39mlimit, datatype\u001b[38;5;241m=\u001b[39mdatatype, no_header\u001b[38;5;241m=\u001b[39mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/gensim/models/keyedvectors.py:2065\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2062\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[38;5;241m=\u001b[39mdatatype)\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m-> 2065\u001b[0m     _word2vec_read_binary(\n\u001b[1;32m   2066\u001b[0m         fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[1;32m   2067\u001b[0m     )\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1958\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1955\u001b[0m tot_processed_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tot_processed_words \u001b[38;5;241m<\u001b[39m vocab_size:\n\u001b[0;32m-> 1958\u001b[0m     new_chunk \u001b[38;5;241m=\u001b[39m fin\u001b[38;5;241m.\u001b[39mread(binary_chunk_size)\n\u001b[1;32m   1959\u001b[0m     chunk \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_chunk\n\u001b[1;32m   1960\u001b[0m     processed_words, chunk \u001b[38;5;241m=\u001b[39m _add_bytes_to_kv(\n\u001b[1;32m   1961\u001b[0m         kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread(size)\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/sermons/lib/python3.11/gzip.py:507\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    505\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 507\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(buf, size)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## expand keywords\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "keyword_temp = full_keywords\n",
    "model_name = 'word2vec-google-news-300'\n",
    "word2vec_model = gensim.downloader.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dba91201-b6b6-4df1-bf3b-1e0a2e2be75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centroid(keywords, model):\n",
    "    vectors = [model[word] for word in keywords if word in model.vocab]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def expand_keywords(keywords, model, centroid, topn=5):\n",
    "    expanded_keywords = {}\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            similar_words = model.most_similar(keyword, topn=topn)\n",
    "            expanded_keywords[keyword] = [word for word, _ in similar_words]\n",
    "        except KeyError:\n",
    "            print(f\"Word '{keyword}' not in vocabulary.\")\n",
    "            expanded_keywords[keyword] = []\n",
    "\n",
    "    return expanded_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fe17240-2861-434e-951b-b5f56dffac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'ephesians' not in vocabulary.\n",
      "Word 'supreme being' not in vocabulary.\n",
      "Word 'jehovah' not in vocabulary.\n",
      "Word 'corinthians' not in vocabulary.\n",
      "Word 'israelites' not in vocabulary.\n",
      "Word 'hebrews' not in vocabulary.\n",
      "Word 'philippians' not in vocabulary.\n",
      "ephesians: []\n",
      "salvation: ['deliverance', 'resurrection', 'eternal_salvation', 'eternal_destiny', 'savior']\n",
      "scripture: ['Scripture', 'scriptures', 'Scriptures', 'Bible', 'New_Testament']\n",
      "supreme being: []\n",
      "god: ['gods', 'deity', 'God', 'diety', 'goddess']\n",
      "eternal: ['everlasting', 'eternity', 'earthly', 'eternal_bliss', 'blessedness']\n",
      "resurrection: ['rebirth', 'miraculous_resurrection', 'salvation', 'bodily_resurrection', 'Jesus_resurrection']\n",
      "psalm: ['Psalm', 'Psalms', 'psalms', 'prophet_Jeremiah', 'verse']\n",
      "righteousness: ['righteous', 'holiness', 'Ps_##:#', 'Matt._#:##-##', 'lovingkindness']\n",
      "christ: ['jesus', 'derek', 'cohen', 'francis', 'moses']\n",
      "righteous: ['righteousness', 'effectual_fervent_prayer', 'virtuous', 'godly', 'self_righteous']\n",
      "disciples: ['apostles', 'disciple', 'Jesus_Himself', 'prophets', 'Luke_##:##-##']\n",
      "heaven: ['Heaven', 'heavenly', 'heavens', 'god', 'God']\n",
      "pharisees: ['Pharisees_hypocrites', 'Pharisees', 'idol_worshipers', 'biblical_literalists', 'Biblical_prophets']\n",
      "believers: ['Christians', 'nonbelievers', 'unbelievers', 'professing_Christians', 'devout_believers']\n",
      "holy: ['Muslim_holy', 'holiest', 'Holy', 'Rabbi_Mitch_Chefitz', 'sacred']\n",
      "repent: ['repented', 'repenting', 'repentance', 'repents', 'sinful']\n",
      "messiah: ['savior', 'Messiah', 'messiahs', 'messianic', 'anti_christ']\n",
      "prophets: ['prophet', 'Old_Testament_prophets', 'biblical_prophets', 'Biblical_prophets', 'false_prophets']\n",
      "sinners: ['sinner', 'sin', 'sins', 'sodomites', 'unbelievers']\n",
      "jehovah: []\n",
      "corinthians: []\n",
      "apostle: ['apostles', 'Apostle', 'apostle_Paul', 'evangelizer', 'disciple']\n",
      "divine: ['heavenly', 'God', 'earthly', 'God_Himself', 'Divine']\n",
      "israelites: []\n",
      "hebrews: []\n",
      "bible: ['Bible', 'bibles', 'Holy_Bible', 'scriptures', 'scripture']\n",
      "lord: ['lords', 'god', 'Lord', 'Thou_knowest', 'Lord_Ackner']\n",
      "philippians: []\n",
      "almighty: ['mighty', 'Almighty', 'god', 'unholy', 'divine']\n",
      "sermon: ['sermons', 'homily', 'fiery_sermon', 'sermon_preached', 'preached_sermon']\n",
      "verse: ['verses', 'lyric', 'poem', 'psalm', 'sonnet']\n"
     ]
    }
   ],
   "source": [
    "non_specific = ['romans', 'maker', 'glory', 'savior', 'creator', 'sins', 'verses', 'thou', 'gospel', 'lord']\n",
    "centroid = calculate_centroid(keyword_temp, word2vec_model)\n",
    "expanded_keywords = expand_keywords([k for k in keyword_temp if k not in non_specific], word2vec_model, centroid, topn=5)\n",
    "\n",
    "for keyword, similar_words in expanded_keywords.items():\n",
    "    print(f\"{keyword}: {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77efcd5-a225-41e0-838b-a4c8f413c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_keywords.remove('God')\n",
    "full_keywords.remove('god')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ebbc091-1fcf-4073-8cab-7e92a6813431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "def get_lexical_overlap(speech):\n",
    "    if len(speech) > 0:\n",
    "        count = 0\n",
    "        for t in full_keywords:\n",
    "            count += speech.count(t)\n",
    "\n",
    "        return count / len(nltk.word_tokenize(speech)) # roughly normalize\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ed52e8c-d3eb-4180-91ad-dfc0980fdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2cff1-7e8c-4553-bb9d-6e45d86346b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "presidential_df = presidential_df[presidential_df['is_president']]\n",
    "presidential_df['lexical'] = presidential_df['text'].progress_apply(get_lexical_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11a4b6-7d50-4fd1-9b28-122c7a697979",
   "metadata": {},
   "outputs": [],
   "source": [
    "presidential_df['year'] = presidential_df['year'].progress_apply(lambda x: int(x))\n",
    "presidential_df = presidential_df[presidential_df['year'] < 2022]\n",
    "presidential_df['inaugural_year'] = presidential_df['year'].progress_apply(lambda x: (x - 1) % 4 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab1339-02a9-40ca-a75d-b742a54f17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = presidential_df[presidential_df['year'] >= 1980]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab2d218-17c7-40f7-a894-f93c57ddc57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47486ab0-0444-4a65-8e54-50df1f3f1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.drop([res.lexical.idxmax(),res.lexical.idxmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26e800-9f7d-4627-bd25-183d5c686b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents = {\n",
    "    1977: 'Jimmy Carter',\n",
    "    1981: 'Ronald Reagan',\n",
    "    1989: 'George Bush',\n",
    "    1993: 'William J. Clinton',\n",
    "    2001: 'George W. Bush',\n",
    "    2009: 'Barack Obama',\n",
    "    2017: 'Donald J. Trump',\n",
    "    2021: 'Joseph R. Biden',\n",
    "    2025: 'unknown' \n",
    "}\n",
    "presidents = {v: k for k, v in presidents.items()}\n",
    "year_vals = sorted(list(presidents.values()))\n",
    "\n",
    "def during_term(row_data):\n",
    "    pres_name = row_data['speaker']\n",
    "    start = presidents[pres_name]\n",
    "    end = year_vals[year_vals.index(presidents[pres_name]) + 1]\n",
    "    \n",
    "    return row_data['year'] >= start and row_data['year'] < end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadb0cd-299c-4e11-9b15-0aa1a9c80093",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['during_term'] = res.apply(during_term, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81c959-2546-42f9-8bb0-70ccf419f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[res['during_term']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69045adb-bc03-4ce2-b8cb-fe2c7c746496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bc528-e4f1-4d2b-8b73-009539ad8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {'Democratic': 'blue', 'Republican': 'red'}\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)}, style='whitegrid')\n",
    "sns.pointplot(data=res, y='lexical', x='year', hue='party', palette=palette, errorbar=None)\n",
    "#sns.lineplot(data=res, y='lexical', x='year', color='black', ci=None)\n",
    "#sns.pointplot(y=pres_num[:,3], x=pres_num[:,0], hue=pres_num[:,4])#, linestyle='none', errorbar=None)\n",
    "plt.xticks(rotation=75)\n",
    "plt.ylabel('Proportion of religious keywords')\n",
    "plt.xlabel('Year')\n",
    "plt.legend(title='Party', bbox_to_anchor=(1.05, 0.8), loc='upper left', borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffeed7-e3f8-4565-8142-78893997f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,8)}, style='whitegrid')\n",
    "sns.pointplot(data=res, y='lexical', x='year', hue='party', join=False)\n",
    "#sns.pointplot(y=pres_num[:,3], x=pres_num[:,0], hue=pres_num[:,4])#, linestyle='none', errorbar=None)\n",
    "plt.xticks(rotation=75)\n",
    "plt.ylabel('Proportion of religious keywords')\n",
    "plt.xlabel('Year')\n",
    "plt.legend(title='Party', bbox_to_anchor=(1.05, 0.8), loc='upper left', borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664b368-71b4-4568-a247-c38068484806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6591e-da4c-4891-8283-78183c156866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sermons",
   "language": "python",
   "name": "sermons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
